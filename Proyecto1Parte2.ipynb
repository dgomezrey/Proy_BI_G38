{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10_Uz0nNAiLJ",
        "outputId": "5117145e-b9fc-482f-ee52-c5ae0346e1b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=727ff52f409247302327bfa5894d95a6c9c476b13a49dec9202f02efa39173df\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2024.10.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2024.10.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=c114e5050a25ea7998b860a29a73e4e4364a2eed6e2ff5137a07be99992ca806\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.10.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting docopt>=0.6.2 (from num2words)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=bb59b861a0db42bca5128fe3720d07ab96b22a4c0895b28a95adfc83708829a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "!pip install num2words\n",
        "!pip install ftfy\n",
        "!pip install nltk\n",
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpFzLrIpQft9"
      },
      "outputs": [],
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OXf091MUTG_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfQ0FeE8gMj7"
      },
      "outputs": [],
      "source": [
        "# Importación de librerías\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "from num2words import num2words\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "import ftfy\n",
        "import joblib\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Descargar recursos de nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Paso 1: Función para detectar idioma y traducir si es necesario\n",
        "def translate_text(text):\n",
        "    translator = Translator()\n",
        "    detected_lang = detect(text)\n",
        "    if detected_lang != 'es':\n",
        "        try:\n",
        "            translation = translator.translate(text, src=detected_lang, dest='es')\n",
        "            return translation.text\n",
        "        except Exception:\n",
        "            return text\n",
        "    return text\n",
        "\n",
        "# Paso 2: Función para limpieza de texto\n",
        "def clean_text(text):\n",
        "    # Convertir a minúsculas\n",
        "    text = text.lower()\n",
        "    # Eliminar caracteres no ASCII\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
        "    # Eliminar signos de puntuación\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Eliminar stopwords\n",
        "    stop_words = set(stopwords.words('spanish'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Paso 3: Corrección de errores de codificación\n",
        "def fix_encoding(text):\n",
        "    return ftfy.fix_text(text)\n",
        "\n",
        "# Paso 4: Conversión de números a palabras\n",
        "def convert_numbers(text):\n",
        "    return ' '.join([num2words(word, lang='es') if word.isdigit() else word for word in text.split()])\n",
        "\n",
        "# Paso 5: Tokenización de palabras\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Paso 6: Aplicar stemming y lematización\n",
        "def stem_and_lemmatize(tokens):\n",
        "    stemmer = SnowballStemmer('spanish')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stems = [stemmer.stem(token) for token in tokens]\n",
        "    lemmas = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
        "    # Concatenar stems y lemmas para mayor cobertura\n",
        "    return stems + lemmas\n",
        "\n",
        "# Paso 7: Preprocesamiento completo del texto\n",
        "def preprocess_text(text):\n",
        "    # Traducción de texto si es necesario\n",
        "    text = translate_text(text)\n",
        "    # Limpieza básica\n",
        "    text = clean_text(text)\n",
        "    # Corrección de codificación\n",
        "    text = fix_encoding(text)\n",
        "    # Conversión de números a palabras\n",
        "    text = convert_numbers(text)\n",
        "    # Tokenización\n",
        "    tokens = tokenize_text(text)\n",
        "    # Stemming y lematización\n",
        "    processed_tokens = stem_and_lemmatize(tokens)\n",
        "    # Unir tokens procesados en una cadena\n",
        "    return ' '.join(processed_tokens)\n",
        "\n",
        "# Clase personalizada para el preprocesamiento de texto\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        # Aplicar el preprocesamiento personalizado a cada texto\n",
        "        return X.apply(preprocess_text)\n",
        "\n",
        "# Paso 8: Cargar los datos\n",
        "data_path = '/content/ODScat_345.xlsx'\n",
        "ods_data = pd.read_excel(data_path)\n",
        "\n",
        "# Paso 9: Verificar la estructura de los datos\n",
        "print(f\"El archivo tiene {ods_data.shape[0]} filas y {ods_data.shape[1]} columnas.\")\n",
        "print(\"\\nPrimeros registros:\")\n",
        "print(ods_data.head())\n",
        "\n",
        "# Paso 10: Verificar si existen duplicados y eliminarlos\n",
        "ods_data = ods_data.drop_duplicates()\n",
        "\n",
        "# Paso 11: Separar características (X) y etiquetas (y)\n",
        "X_data = ods_data['Textos_espanol']\n",
        "y_data = ods_data['sdg']\n",
        "\n",
        "# Paso 12: División del conjunto de datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=42)\n",
        "\n",
        "# Paso 13: Definir el pipeline con el preprocesador personalizado, TF-IDF y Naive Bayes\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', TextPreprocessor()), # Transformador personalizado para limpieza\n",
        "    ('vectorizer', TfidfVectorizer()),  # Utiliza TfidfVectorizer para transformar el texto en características numéricas\n",
        "    ('classifier', MultinomialNB())     # Utiliza el clasificador Naive Bayes\n",
        "])\n",
        "\n",
        "# Paso 14: Entrenamiento del modelo\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Paso 15: Predicción y evaluación\n",
        "y_pred = pipeline.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nExactitud del modelo: {accuracy:.4f}\")\n",
        "print(\"\\nReporte de clasificación:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Paso 16: Guardar el modelo entrenado para uso en la API\n",
        "joblib.dump(pipeline, 'modelo_entrenado_completo.joblib')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HTnj4NFGImC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Paso 21: Calcular la matriz de confusión\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Paso 22: Graficar la matriz de confusión\n",
        "plt.figure(figsize=(8, 6))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipeline.classes_)\n",
        "disp.plot(cmap='Blues', values_format='d')\n",
        "plt.title('Matriz de Confusión del Modelo')\n",
        "plt.xlabel('Etiqueta Predicha')\n",
        "plt.ylabel('Etiqueta Verdadera')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW1UxvI3Lqdz"
      },
      "source": [
        "Prediccion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMIbBWtxhynH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from joblib import load\n",
        "\n",
        "# Cargar el modelo entrenado\n",
        "pipeline = load('modelo_entrenado_completo.joblib')\n",
        "\n",
        "# Cargar los datos de prueba\n",
        "test_data_path = '/content/TestODScat_345.xlsx'\n",
        "test_data = pd.read_excel(test_data_path)\n",
        "\n",
        "# Asegurarse de que la columna con los textos se llame 'Textos_espanol'\n",
        "# Si no es así, ajusta el nombre de la columna aquí.\n",
        "if 'Textos_espanol' not in test_data.columns:\n",
        "    print(\"Error: La columna 'Textos_espanol' no se encontró en el archivo de prueba.\")\n",
        "else:\n",
        "    # Realizar la predicción con el modelo cargado\n",
        "    predictions = pipeline.predict(test_data['Textos_espanol'])\n",
        "\n",
        "    # Agregar las predicciones al DataFrame\n",
        "    test_data['Predicciones'] = predictions\n",
        "\n",
        "    # Mostrar los resultados\n",
        "    print(test_data[['Textos_espanol', 'Predicciones']])\n",
        "\n",
        "    # Guardar el DataFrame con las predicciones en un archivo (opcional)\n",
        "    test_data.to_excel('resultados_predicciones.xlsx', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}